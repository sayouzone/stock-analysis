{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a37a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "import tempfile\n",
    "import zipfile\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import (\n",
    "    ConnectionError,\n",
    "\tRetryError,\n",
    ")\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "PROJECT_ROOT = project_root\n",
    "\n",
    "DATASET_DIR = os.path.join(project_root, 'datasets')\n",
    "\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "\tos.mkdir(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c53cf4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_indices(\n",
    "    start_year: int,\n",
    "    end_year: int,\n",
    "    quarters: List[int],\n",
    "    skip_present_indices: bool,\n",
    "    indices_folder: str,\n",
    "    user_agent: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Downloads EDGAR Index files for the specified years and quarters.\n",
    "\n",
    "    Args:\n",
    "            start_year (int): The first year of the indices to be downloaded.\n",
    "            end_year (int): The last year of the indices to be downloaded.\n",
    "            quarters (List[int]): A list of quarters (in the format 1, 2, 3, 4) for which the indices will be downloaded.\n",
    "            skip_present_indices (bool): If True, the function will skip downloading indices that are already present in the directory.\n",
    "            indices_folder (str): Directory where the indices will be saved.\n",
    "            user_agent (str): The User-Agent string that will be declared to SEC EDGAR.\n",
    "\n",
    "    Raises:\n",
    "            ValueError: If an invalid quarter is passed.\n",
    "    \"\"\"\n",
    "\n",
    "    base_url = \"https://www.sec.gov/Archives/edgar/full-index/\"\n",
    "\n",
    "    print(\"Downloading index files from SEC...\")\n",
    "\n",
    "    for quarter in quarters:\n",
    "        if quarter not in [1, 2, 3, 4]:\n",
    "            raise Exception(f'Invalid quarter \"{quarter}\"')\n",
    "    \n",
    "    first_iteration = True\n",
    "    # Loop over the years and quarters to download the indices\n",
    "    while True:\n",
    "        failed_indices = []\n",
    "        for year in range(start_year, end_year+1):\n",
    "            for quarter in quarters:\n",
    "                if year == datetime.now().year and quarter > math.ceil(\n",
    "                    datetime.now().month / 3\n",
    "                ): # Skip future quarters\n",
    "                    break\n",
    "                    \n",
    "                index_filename = f\"{year}_QTR{quarter}.tsv\"\n",
    "\n",
    "                # Check if the index file is already present\n",
    "                if skip_present_indices and os.path.exists(\n",
    "                    os.path.join(indices_folder, index_filename)\n",
    "                ):\n",
    "                    if first_iteration:\n",
    "                        print(f\"Skipping {index_filename}\")\n",
    "                    continue\n",
    "\n",
    "                # If not, download the index file\n",
    "                url = f\"{base_url}/{year}/QTR{quarter}/master.zip\"\n",
    "                \n",
    "                # Retry the download in case of failures\n",
    "                with tempfile.TemporaryFile(mode=\"w+b\") as tmp:\n",
    "                    try:\n",
    "                        request = requests.get(url=url, headers={\"User-agent\": user_agent})\n",
    "                    except Exception as e:\n",
    "                        failed_indices.append(index_filename)\n",
    "                        continue\n",
    "                \n",
    "                    \n",
    "                    tmp.write(request.content)\n",
    "\n",
    "                    with zipfile.ZipFile(tmp).open(\"master.idx\") as f:\n",
    "                        lines = [\n",
    "                            (decoded := line.decode(\"latin-1\")).strip() \n",
    "                            + \"|\" + decoded.split(\"|\")[-1].replace(\".txt\", \"-index.html\")\n",
    "                            for line in itertools.islice(f, 11, None)\n",
    "                        ]\n",
    "\n",
    "                    # Save the processed index file\n",
    "                    with open(\n",
    "                        os.path.join(indices_folder, index_filename),\n",
    "                        \"w+\",\n",
    "                        encoding=\"utf-8\",\n",
    "                    ) as f:\n",
    "                        f.write(\"\".join(lines))\n",
    "                        print(f\"{index_filename} downloaded\")\n",
    "        first_iteration = False\n",
    "        # Handle failed downloads\n",
    "        if len(failed_indices) > 0:\n",
    "            print(f\"Could not download the following indices:\\n{failed_indices}\")\n",
    "            user_input = input(\"Retry (Y/N): \")\n",
    "            if user_input in [\"Y\", \"y\", \"yes\"]:\n",
    "                print(\"Retry downloading failed indices\")\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad028d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Orchestrates the entire flow of crawling and downloading filings from SEC EDGAR.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads the configuration file.\n",
    "    2. Creates necessary directories.\n",
    "    3. Filters out the unnecessary years.\n",
    "    4. Downloads the indices.\n",
    "    5. Gets specific indices according to the provided filing types and CIKs/tickers.\n",
    "    6. Compares the new indices with the old ones to download only the new filings.\n",
    "    7. Crawls through each index to download (.tsv files) and save the filing.\n",
    "\n",
    "    Raises:\n",
    "            SystemExit: If no filing types are provided or if there are no new filings to download.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the configuration file\n",
    "    config_path = os.path.join(PROJECT_ROOT, \"config.json\")\n",
    "    with open(config_path) as fin:\n",
    "        config = json.load(fin)[\"download_filings\"]\n",
    "\n",
    "    # Define the directories and filepaths\n",
    "    raw_filings_folder = os.path.join(DATASET_DIR, config[\"raw_filings_folder\"])\n",
    "    indices_folder = os.path.join(DATASET_DIR, config[\"indices_folder\"])\n",
    "    filings_metadata_filepath = os.path.join(\n",
    "        DATASET_DIR, config[\"filings_metadata_file\"]\n",
    "    )\n",
    "    \n",
    "    # Check if at least one filing type is provided\n",
    "    if len(config[\"filing_types\"]) == 0:\n",
    "        print(\"Please provide at least one filing type\")\n",
    "        exit()\n",
    "\n",
    "    # If the indices and/or download folder doesn't exist, create them\n",
    "    if not os.path.isdir(indices_folder):\n",
    "        os.mkdir(indices_folder)\n",
    "    if not os.path.isdir(raw_filings_folder):\n",
    "        os.mkdir(raw_filings_folder)\n",
    "\n",
    "    # We also create subfolders for each filing type in the raw_filings_folder for better organization\n",
    "    for filing_type in config[\"filing_types\"]:\n",
    "        filing_type_folder = os.path.join(raw_filings_folder, filing_type)\n",
    "        if not os.path.isdir(filing_type_folder):\n",
    "            os.mkdir(filing_type_folder)\n",
    "\n",
    "    # If companies_info.json doesn't exist, create it with empty JSON\n",
    "    if not os.path.isfile(os.path.join(DATASET_DIR, \"companies_info.json\")):\n",
    "        with open(os.path.join(DATASET_DIR, \"companies_info.json\"), \"w\") as f:\n",
    "            json.dump(obj={}, fp=f)\n",
    "    \n",
    "    download_indices(\n",
    "        start_year=config[\"start_year\"],\n",
    "        end_year=config[\"end_year\"],\n",
    "        quarters=config[\"quarters\"],\n",
    "        skip_present_indices=config[\"skip_present_indices\"],\n",
    "        indices_folder=indices_folder,\n",
    "        user_agent=config[\"user_agent\"],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
